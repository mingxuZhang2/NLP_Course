test_loss: 0.391, test_acc: 0.822
hyper-parameters: Namespace(max_length=256, test_size=0.25, min_freq=5, batch_size=512, embedding_dim=300, n_filters=100, filter_sizes=[3, 5, 7], dropout_rate=0.3, n_epochs=40, device='cuda:1')
metrics: defaultdict(<class 'list'>, {'train_losses': [np.float64(0.7401900758614411), np.float64(0.5605643762124551), np.float64(0.4834008079928321), np.float64(0.43194989980878057)], 'train_accs': [np.float64(0.5848654879106058), np.float64(0.7185581068734865), np.float64(0.7662196014378522), np.float64(0.7949839572648745)], 'valid_losses': [np.float64(0.5063611131448013), np.float64(0.4524365778152759), np.float64(0.419320679627932), np.float64(0.3907939768754519)], 'valid_accs': [np.float64(0.7529424428939819), np.float64(0.7820607332082895), np.float64(0.7995651547725384), np.float64(0.8150143990149865)]})
vocab: Vocab()
model: CNN(
  (embedding): Embedding(21602, 300, padding_idx=1)
  (convs): ModuleList(
    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))
    (1): Conv1d(300, 100, kernel_size=(5,), stride=(1,))
    (2): Conv1d(300, 100, kernel_size=(7,), stride=(1,))
  )
  (fc): Linear(in_features=300, out_features=2, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
criterion: CrossEntropyLoss()
device: cuda:1
special_tokens: ['<unk>', '<pad>']
unk_index: 0
pad_index: 1
vocab_size: 21602
output_dim: 2
