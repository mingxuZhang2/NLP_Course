test_loss: 0.397, test_acc: 0.819
hyper-parameters: Namespace(max_length=256, test_size=0.25, min_freq=5, batch_size=512, embedding_dim=300, n_filters=100, filter_sizes=[3, 5, 7], dropout_rate=0.3, n_epochs=40, device='cuda:2')
metrics: defaultdict(<class 'list'>, {'train_losses': [np.float64(0.7410314969114355), np.float64(0.5597851437491339), np.float64(0.4908317883272429), np.float64(0.43586357541986415)], 'train_accs': [np.float64(0.5786767746951129), np.float64(0.715769350528717), np.float64(0.7610633920978855), np.float64(0.7934441663123466)], 'valid_losses': [np.float64(0.5280253749627334), np.float64(0.450904885163674), np.float64(0.42135942670015186), np.float64(0.3953399360179901)], 'valid_accs': [np.float64(0.746054062476525), np.float64(0.7831606039634118), np.float64(0.7993610547139094), np.float64(0.819144592835353)]})
vocab: Vocab()
model: CNN(
  (embedding): Embedding(21538, 300, padding_idx=1)
  (convs): ModuleList(
    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))
    (1): Conv1d(300, 100, kernel_size=(5,), stride=(1,))
    (2): Conv1d(300, 100, kernel_size=(7,), stride=(1,))
  )
  (fc): Linear(in_features=300, out_features=2, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
criterion: CrossEntropyLoss()
device: cuda:2
special_tokens: ['<unk>', '<pad>']
unk_index: 0
pad_index: 1
vocab_size: 21538
output_dim: 2
